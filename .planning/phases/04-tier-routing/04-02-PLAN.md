---
phase: 04-tier-routing
plan: 02
type: execute
wave: 1
depends_on: ["04-01", "04-01b"]
files_modified:
  - src/tiers/health.rs
  - src/tiers/router.rs
  - src/tiers/mod.rs
  - src/routes/metrics.rs
autonomous: true

must_haves:
  truths:
    - "ProviderHealthTracker tracks availability with exponential backoff"
    - "Cost-weighted selection favors cheaper models probabilistically"
    - "TierRouter selects model for a tier considering health and cost"
    - "Request succeeds with alternative model when primary model fails (if available)"
    - "Prometheus metrics track tier requests and model selections"
  artifacts:
    - path: "src/tiers/health.rs"
      provides: "ProviderHealthTracker with exponential backoff"
      exports: ["ProviderHealthTracker", "HealthConfig"]
    - path: "src/tiers/router.rs"
      provides: "TierRouter for model selection"
      exports: ["TierRouter", "SelectedModel"]
    - path: "src/routes/metrics.rs"
      provides: "Tier routing metrics"
      contains: "sentinel_tier_requests_total"
  key_links:
    - from: "src/tiers/router.rs"
      to: "src/tiers/health.rs"
      via: "Health check during selection"
      pattern: "health_tracker\\.is_available"
    - from: "src/tiers/router.rs"
      to: "src/tiers/cache.rs"
      via: "Config lookup"
      pattern: "config_cache\\.get_config"
---

<objective>
Create the model selection logic: ProviderHealthTracker with exponential backoff and TierRouter with cost-weighted selection.

Purpose: Implement the core routing algorithm that selects models based on tier, cost, and health. This is the engine that Plan 03 will integrate into the request handler.

Output: TierRouter service that can select optimal models for tiers, with health tracking and cost-based weighting.
</objective>

<execution_context>
@/Users/gregor/.claude/get-shit-done/workflows/execute-plan.md
@/Users/gregor/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-tier-routing/04-CONTEXT.md
@.planning/phases/04-tier-routing/04-RESEARCH.md
@.planning/phases/04-tier-routing/04-01-SUMMARY.md

@src/tiers/mod.rs
@src/tiers/config.rs
@src/tiers/cache.rs
@src/zion/models.rs
@src/routes/metrics.rs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create ProviderHealthTracker</name>
  <files>src/tiers/health.rs, src/tiers/mod.rs</files>
  <action>
Create `src/tiers/health.rs` with provider health tracking and exponential backoff:

```rust
//! Provider health tracking with exponential backoff
//!
//! Tracks provider/model availability and implements exponential backoff
//! when failures occur. This enables graceful degradation during outages.

use std::collections::HashMap;
use std::sync::RwLock;
use std::time::{Duration, Instant};

use tracing::{debug, info, warn};

/// Configuration for health tracking
#[derive(Debug, Clone)]
pub struct HealthConfig {
    /// Initial backoff duration after first failure (default: 30 seconds)
    pub initial_backoff: Duration,
    /// Maximum backoff duration (default: 5 minutes)
    pub max_backoff: Duration,
    /// Multiplier for exponential backoff (default: 2.0)
    pub backoff_multiplier: f64,
}

impl Default for HealthConfig {
    fn default() -> Self {
        Self {
            initial_backoff: Duration::from_secs(30),
            max_backoff: Duration::from_secs(300), // 5 minutes
            backoff_multiplier: 2.0,
        }
    }
}

/// Health state for a provider/model combination
#[derive(Debug, Clone)]
struct HealthState {
    /// Whether currently considered available (before backoff check)
    available: bool,
    /// Time of last failure (for backoff calculation)
    last_failure: Option<Instant>,
    /// Current backoff duration
    backoff_duration: Duration,
    /// Number of consecutive failures
    consecutive_failures: u32,
}

impl Default for HealthState {
    fn default() -> Self {
        Self {
            available: true,
            last_failure: None,
            backoff_duration: Duration::from_secs(30),
            consecutive_failures: 0,
        }
    }
}

/// Tracks health of provider/model combinations
///
/// Uses exponential backoff to avoid hammering unhealthy providers.
/// State is kept in-memory per instance (each Sentinel instance
/// independently tracks health based on its own observations).
pub struct ProviderHealthTracker {
    states: RwLock<HashMap<(String, String), HealthState>>,
    config: HealthConfig,
}

impl ProviderHealthTracker {
    /// Create a new health tracker with default configuration
    pub fn new() -> Self {
        Self::with_config(HealthConfig::default())
    }

    /// Create a new health tracker with custom configuration
    pub fn with_config(config: HealthConfig) -> Self {
        Self {
            states: RwLock::new(HashMap::new()),
            config,
        }
    }

    /// Check if a provider/model is currently available
    ///
    /// Returns true if:
    /// - Never seen before (unknown = available)
    /// - Currently healthy
    /// - In backoff but backoff period has elapsed (ready to retry)
    pub fn is_available(&self, provider: &str, model: &str) -> bool {
        let key = (provider.to_string(), model.to_string());
        let states = self.states.read().unwrap();

        match states.get(&key) {
            None => true, // Unknown = available
            Some(state) => {
                if state.available {
                    return true;
                }
                // Check if backoff period has elapsed
                if let Some(last_failure) = state.last_failure {
                    if last_failure.elapsed() >= state.backoff_duration {
                        debug!(
                            provider = %provider,
                            model = %model,
                            backoff_secs = state.backoff_duration.as_secs(),
                            "Backoff elapsed, provider ready for retry"
                        );
                        return true; // Ready to retry
                    }
                }
                false
            }
        }
    }

    /// Get remaining backoff time for a provider/model (for Retry-After header)
    pub fn backoff_remaining(&self, provider: &str, model: &str) -> Option<Duration> {
        let key = (provider.to_string(), model.to_string());
        let states = self.states.read().unwrap();

        states.get(&key).and_then(|state| {
            if state.available {
                return None;
            }
            state.last_failure.map(|last| {
                let elapsed = last.elapsed();
                if elapsed >= state.backoff_duration {
                    Duration::ZERO
                } else {
                    state.backoff_duration - elapsed
                }
            })
        })
    }

    /// Record a successful request (reset backoff)
    pub fn record_success(&self, provider: &str, model: &str) {
        let key = (provider.to_string(), model.to_string());
        let mut states = self.states.write().unwrap();

        if let Some(state) = states.get(&key) {
            if !state.available || state.consecutive_failures > 0 {
                info!(
                    provider = %provider,
                    model = %model,
                    previous_failures = state.consecutive_failures,
                    "Provider recovered, resetting health state"
                );
            }
        }

        states.insert(key, HealthState::default());
    }

    /// Record a failure (apply exponential backoff)
    pub fn record_failure(&self, provider: &str, model: &str) {
        let key = (provider.to_string(), model.to_string());
        let mut states = self.states.write().unwrap();

        let state = states.entry(key).or_default();
        state.available = false;
        state.last_failure = Some(Instant::now());
        state.consecutive_failures += 1;

        // Calculate exponential backoff: initial * multiplier^(failures-1)
        let backoff_secs = self.config.initial_backoff.as_secs_f64()
            * self.config.backoff_multiplier.powi(state.consecutive_failures as i32 - 1);
        let new_backoff = Duration::from_secs_f64(backoff_secs);
        state.backoff_duration = new_backoff.min(self.config.max_backoff);

        warn!(
            provider = %provider,
            model = %model,
            consecutive_failures = state.consecutive_failures,
            backoff_secs = state.backoff_duration.as_secs(),
            "Provider failure recorded, entering backoff"
        );
    }

    /// Get current state summary for debugging/metrics
    pub fn get_unavailable_providers(&self) -> Vec<(String, String, u32)> {
        let states = self.states.read().unwrap();
        states
            .iter()
            .filter(|(_, state)| !state.available)
            .map(|((provider, model), state)| {
                (provider.clone(), model.clone(), state.consecutive_failures)
            })
            .collect()
    }
}

impl Default for ProviderHealthTracker {
    fn default() -> Self {
        Self::new()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::thread::sleep;

    #[test]
    fn test_new_provider_is_available() {
        let tracker = ProviderHealthTracker::new();
        assert!(tracker.is_available("openai", "gpt-4o"));
    }

    #[test]
    fn test_failure_marks_unavailable() {
        let tracker = ProviderHealthTracker::new();
        tracker.record_failure("openai", "gpt-4o");
        assert!(!tracker.is_available("openai", "gpt-4o"));
    }

    #[test]
    fn test_success_resets_state() {
        let tracker = ProviderHealthTracker::new();
        tracker.record_failure("openai", "gpt-4o");
        assert!(!tracker.is_available("openai", "gpt-4o"));

        tracker.record_success("openai", "gpt-4o");
        assert!(tracker.is_available("openai", "gpt-4o"));
    }

    #[test]
    fn test_backoff_elapsed_makes_available() {
        let config = HealthConfig {
            initial_backoff: Duration::from_millis(50),
            max_backoff: Duration::from_secs(1),
            backoff_multiplier: 2.0,
        };
        let tracker = ProviderHealthTracker::with_config(config);

        tracker.record_failure("openai", "gpt-4o");
        assert!(!tracker.is_available("openai", "gpt-4o"));

        // Wait for backoff to elapse
        sleep(Duration::from_millis(60));
        assert!(tracker.is_available("openai", "gpt-4o"));
    }

    #[test]
    fn test_exponential_backoff_increases() {
        let config = HealthConfig {
            initial_backoff: Duration::from_secs(10),
            max_backoff: Duration::from_secs(300),
            backoff_multiplier: 2.0,
        };
        let tracker = ProviderHealthTracker::with_config(config);

        // First failure: 10s backoff
        tracker.record_failure("openai", "gpt-4o");
        let remaining1 = tracker.backoff_remaining("openai", "gpt-4o").unwrap();
        assert!(remaining1.as_secs() <= 10);

        // Second failure: 20s backoff
        tracker.record_failure("openai", "gpt-4o");
        let remaining2 = tracker.backoff_remaining("openai", "gpt-4o").unwrap();
        assert!(remaining2.as_secs() <= 20);
        assert!(remaining2.as_secs() > 10);
    }

    #[test]
    fn test_backoff_capped_at_max() {
        let config = HealthConfig {
            initial_backoff: Duration::from_secs(100),
            max_backoff: Duration::from_secs(150),
            backoff_multiplier: 2.0,
        };
        let tracker = ProviderHealthTracker::with_config(config);

        // Multiple failures should cap at max
        for _ in 0..10 {
            tracker.record_failure("openai", "gpt-4o");
        }
        let remaining = tracker.backoff_remaining("openai", "gpt-4o").unwrap();
        assert!(remaining.as_secs() <= 150);
    }

    #[test]
    fn test_different_providers_tracked_separately() {
        let tracker = ProviderHealthTracker::new();

        tracker.record_failure("openai", "gpt-4o");
        assert!(!tracker.is_available("openai", "gpt-4o"));
        assert!(tracker.is_available("openai", "gpt-4o-mini")); // Different model
        assert!(tracker.is_available("anthropic", "claude-3")); // Different provider
    }

    #[test]
    fn test_get_unavailable_providers() {
        let tracker = ProviderHealthTracker::new();
        tracker.record_failure("openai", "gpt-4o");
        tracker.record_failure("openai", "gpt-4o"); // 2 failures

        let unavailable = tracker.get_unavailable_providers();
        assert_eq!(unavailable.len(), 1);
        assert_eq!(unavailable[0], ("openai".to_string(), "gpt-4o".to_string(), 2));
    }
}
```

Update `src/tiers/mod.rs` to include the health module:
```rust
pub mod health;
// ... existing exports ...
pub use health::{HealthConfig, ProviderHealthTracker};
```
  </action>
  <verify>
`cargo check` passes.
`cargo test tiers::health::tests` passes with all health tracking tests.
  </verify>
  <done>
ProviderHealthTracker implements exponential backoff.
Backoff starts at 30s, doubles on each failure, caps at 5 minutes.
Success resets the backoff state completely.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create TierRouter with cost-weighted selection</name>
  <files>src/tiers/router.rs, src/tiers/mod.rs</files>
  <action>
Create `src/tiers/router.rs` with the model selection logic:

```rust
//! Tier-based model routing
//!
//! Selects models for tiers using cost-weighted probabilistic selection
//! with health-aware filtering.

use std::sync::Arc;

use rand::distributions::WeightedIndex;
use rand::prelude::*;
use tracing::{debug, info, warn};

use crate::{
    error::{AppError, AppResult},
    native::types::Tier,
    zion::models::ModelConfig,
};

use super::{
    cache::TierConfigCache,
    health::ProviderHealthTracker,
};

/// Result of model selection
#[derive(Debug, Clone)]
pub struct SelectedModel {
    /// Provider name (e.g., "openai")
    pub provider: String,
    /// Model identifier (e.g., "gpt-4o-mini")
    pub model: String,
    /// The tier this model serves
    pub tier: Tier,
}

/// Tier-based model router
///
/// Selects models for complexity tiers using:
/// 1. Health-aware filtering (skip unavailable providers)
/// 2. Cost-weighted probabilistic selection (favor cheaper options)
/// 3. Single retry with next model on failure
pub struct TierRouter {
    config_cache: Arc<TierConfigCache>,
    health_tracker: Arc<ProviderHealthTracker>,
}

impl TierRouter {
    /// Create a new tier router
    pub fn new(
        config_cache: Arc<TierConfigCache>,
        health_tracker: Arc<ProviderHealthTracker>,
    ) -> Self {
        Self {
            config_cache,
            health_tracker,
        }
    }

    /// Select a model for the given tier
    ///
    /// Returns the selected model considering health and cost.
    /// If all models are unavailable, returns ServiceUnavailable error.
    ///
    /// # Arguments
    /// * `tier` - The complexity tier to select a model for
    /// * `preferred_provider` - Optional provider to prefer (for session continuity)
    pub async fn select_model(
        &self,
        tier: Tier,
        preferred_provider: Option<&str>,
    ) -> AppResult<SelectedModel> {
        let config = self.config_cache.get_config().await?;
        let models = config.models_for_tier(tier);

        if models.is_empty() {
            return Err(AppError::BadRequest(format!(
                "No models configured for tier {:?}",
                tier
            )));
        }

        // Filter to healthy models
        let healthy_models: Vec<&ModelConfig> = models
            .iter()
            .filter(|m| self.health_tracker.is_available(&m.provider, &m.model))
            .collect();

        if healthy_models.is_empty() {
            // All models in backoff - find shortest remaining backoff for Retry-After
            let min_backoff = models
                .iter()
                .filter_map(|m| self.health_tracker.backoff_remaining(&m.provider, &m.model))
                .min();

            warn!(
                tier = %tier,
                total_models = models.len(),
                "All models unavailable for tier"
            );

            return Err(AppError::ServiceUnavailable {
                message: format!("All models for tier {} are currently unavailable", tier),
                retry_after: min_backoff,
            });
        }

        // If preferred provider is specified and available, try to use it
        if let Some(preferred) = preferred_provider {
            if let Some(model) = healthy_models.iter().find(|m| m.provider == preferred) {
                debug!(
                    tier = %tier,
                    provider = %model.provider,
                    model = %model.model,
                    "Selected preferred provider"
                );
                return Ok(SelectedModel {
                    provider: model.provider.clone(),
                    model: model.model.clone(),
                    tier,
                });
            }
            debug!(
                tier = %tier,
                preferred = %preferred,
                "Preferred provider not available, falling back to weighted selection"
            );
        }

        // Cost-weighted selection
        let selected = self.select_weighted(&healthy_models)?;

        info!(
            tier = %tier,
            provider = %selected.provider,
            model = %selected.model,
            healthy_count = healthy_models.len(),
            total_count = models.len(),
            "Selected model for tier"
        );

        Ok(SelectedModel {
            provider: selected.provider.clone(),
            model: selected.model.clone(),
            tier,
        })
    }

    /// Select a model using cost-weighted random selection
    ///
    /// Lower relative_cost = higher probability of selection.
    /// Weight = 1 / relative_cost
    fn select_weighted(&self, models: &[&ModelConfig]) -> AppResult<&ModelConfig> {
        if models.is_empty() {
            return Err(AppError::BadRequest("No models available".to_string()));
        }

        if models.len() == 1 {
            return Ok(models[0]);
        }

        // Calculate weights: inverse of cost (lower cost = higher weight)
        // Validate relative_cost >= 1 to avoid division by zero
        let weights: Vec<f64> = models
            .iter()
            .map(|m| {
                let cost = m.relative_cost.max(1) as f64;
                1.0 / cost
            })
            .collect();

        let dist = WeightedIndex::new(&weights).map_err(|e| {
            AppError::Internal(format!("Failed to create weighted distribution: {}", e))
        })?;

        let mut rng = thread_rng();
        let index = dist.sample(&mut rng);

        Ok(models[index])
    }

    /// Get an alternative model for retry after failure
    ///
    /// Returns a different model from the same tier if available.
    /// Excludes the failed model and unhealthy models.
    pub async fn get_retry_model(
        &self,
        tier: Tier,
        failed_model: &str,
    ) -> AppResult<Option<SelectedModel>> {
        let config = self.config_cache.get_config().await?;
        let models = config.models_for_tier(tier);

        // Filter to healthy models that aren't the failed one
        let alternatives: Vec<&ModelConfig> = models
            .iter()
            .filter(|m| {
                m.model != failed_model && self.health_tracker.is_available(&m.provider, &m.model)
            })
            .collect();

        if alternatives.is_empty() {
            debug!(
                tier = %tier,
                failed_model = %failed_model,
                "No alternative models available for retry"
            );
            return Ok(None);
        }

        let selected = self.select_weighted(&alternatives)?;

        info!(
            tier = %tier,
            failed_model = %failed_model,
            retry_provider = %selected.provider,
            retry_model = %selected.model,
            "Selected alternative model for retry"
        );

        Ok(Some(SelectedModel {
            provider: selected.provider.clone(),
            model: selected.model.clone(),
            tier,
        }))
    }

    /// Record a successful request for a model
    pub fn record_success(&self, provider: &str, model: &str) {
        self.health_tracker.record_success(provider, model);
    }

    /// Record a failed request for a model
    pub fn record_failure(&self, provider: &str, model: &str) {
        self.health_tracker.record_failure(provider, model);
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    // Note: Full tests require mocking TierConfigCache.
    // These tests verify the weight calculation logic.

    #[test]
    fn test_weight_calculation_inverse_of_cost() {
        // Relative cost 1 -> weight 1.0
        // Relative cost 2 -> weight 0.5
        // Relative cost 5 -> weight 0.2
        // This means cost=1 is 5x more likely than cost=5

        let weights: Vec<f64> = vec![1, 2, 5]
            .iter()
            .map(|cost| 1.0 / (*cost as f64))
            .collect();

        assert!((weights[0] - 1.0).abs() < 0.001);
        assert!((weights[1] - 0.5).abs() < 0.001);
        assert!((weights[2] - 0.2).abs() < 0.001);
    }

    #[test]
    fn test_zero_cost_handled() {
        // relative_cost of 0 should be treated as 1 to avoid division by zero
        let cost: u8 = 0;
        let safe_cost = cost.max(1) as f64;
        let weight = 1.0 / safe_cost;
        assert!((weight - 1.0).abs() < 0.001);
    }

    #[test]
    fn test_selected_model_debug() {
        let selected = SelectedModel {
            provider: "openai".to_string(),
            model: "gpt-4o".to_string(),
            tier: Tier::Moderate,
        };
        let debug_str = format!("{:?}", selected);
        assert!(debug_str.contains("openai"));
        assert!(debug_str.contains("gpt-4o"));
    }
}
```

Update `src/tiers/mod.rs`:
```rust
pub mod router;
// ... existing exports ...
pub use router::{SelectedModel, TierRouter};
```
  </action>
  <verify>
`cargo check` passes.
`cargo test tiers::router::tests` passes.
  </verify>
  <done>
TierRouter selects models using cost-weighted random selection.
Health-aware filtering skips unavailable providers.
Retry support with get_retry_model.
  </done>
</task>

<task type="auto">
  <name>Task 3: Add ServiceUnavailable error variant</name>
  <files>src/error.rs</files>
  <action>
Add ServiceUnavailable variant to AppError in `src/error.rs`:

```rust
/// Service temporarily unavailable (e.g., all providers in backoff)
#[error("Service unavailable: {message}")]
ServiceUnavailable {
    message: String,
    retry_after: Option<std::time::Duration>,
},
```

Update the IntoResponse implementation to return 503 with Retry-After header:

In the `impl IntoResponse for AppError` block, add the match arm:
```rust
AppError::ServiceUnavailable { message, retry_after } => {
    let mut response = (StatusCode::SERVICE_UNAVAILABLE, Json(json!({
        "error": {
            "message": message,
            "type": "service_unavailable",
            "code": "service_unavailable"
        }
    }))).into_response();

    if let Some(duration) = retry_after {
        if let Ok(value) = HeaderValue::from_str(&duration.as_secs().to_string()) {
            response.headers_mut().insert("Retry-After", value);
        }
    }

    response
}
```

Add necessary imports at the top of error.rs if not present:
```rust
use axum::http::HeaderValue;
```
  </action>
  <verify>
`cargo check` passes.
  </verify>
  <done>
ServiceUnavailable error returns 503 with optional Retry-After header.
  </done>
</task>

<task type="auto">
  <name>Task 4: Add tier routing metrics</name>
  <files>src/routes/metrics.rs</files>
  <action>
Add tier routing metrics to `src/routes/metrics.rs`:

1. Add metric descriptions in `register_metrics()`:
```rust
// Tier routing metrics
metrics::describe_counter!(
    "sentinel_tier_requests_total",
    "Total requests by tier"
);
metrics::describe_counter!(
    "sentinel_model_selections_total",
    "Model selections by tier, provider, and model"
);
metrics::describe_counter!(
    "sentinel_provider_failures_total",
    "Provider failures triggering backoff"
);
metrics::describe_counter!(
    "sentinel_model_retries_total",
    "Model retry attempts after initial failure"
);
metrics::describe_gauge!(
    "sentinel_provider_health",
    "Provider health status (1=healthy, 0=in backoff)"
);
```

2. Add helper functions:
```rust
/// Record a tier request
pub fn record_tier_request(tier: &str) {
    metrics::counter!(
        "sentinel_tier_requests_total",
        "tier" => tier.to_string()
    )
    .increment(1);
}

/// Record model selection
pub fn record_model_selection(tier: &str, provider: &str, model: &str) {
    metrics::counter!(
        "sentinel_model_selections_total",
        "tier" => tier.to_string(),
        "provider" => provider.to_string(),
        "model" => model.to_string()
    )
    .increment(1);
}

/// Record provider failure
pub fn record_provider_failure(provider: &str, model: &str) {
    metrics::counter!(
        "sentinel_provider_failures_total",
        "provider" => provider.to_string(),
        "model" => model.to_string()
    )
    .increment(1);
}

/// Record model retry attempt
pub fn record_model_retry(tier: &str, failed_model: &str, retry_model: &str) {
    metrics::counter!(
        "sentinel_model_retries_total",
        "tier" => tier.to_string(),
        "failed_model" => failed_model.to_string(),
        "retry_model" => retry_model.to_string()
    )
    .increment(1);
}

/// Update provider health gauge
pub fn set_provider_health(provider: &str, model: &str, healthy: bool) {
    metrics::gauge!(
        "sentinel_provider_health",
        "provider" => provider.to_string(),
        "model" => model.to_string()
    )
    .set(if healthy { 1.0 } else { 0.0 });
}
```
  </action>
  <verify>
`cargo check` passes.
`cargo test routes::metrics::tests` passes.
  </verify>
  <done>
Prometheus metrics defined for tier routing observability.
Helper functions ready for use in handler integration.
  </done>
</task>

<task type="auto">
  <name>Task 5: Update tiers module exports</name>
  <files>src/tiers/mod.rs, src/lib.rs</files>
  <action>
Finalize the tiers module in `src/tiers/mod.rs`:

```rust
//! Tier routing module
//!
//! Handles mapping complexity tiers to AI models based on configuration from Zion.
//! Uses cost-weighted selection with health-aware filtering.

pub mod cache;
pub mod config;
pub mod health;
pub mod router;

pub use cache::TierConfigCache;
pub use config::TierConfig;
pub use health::{HealthConfig, ProviderHealthTracker};
pub use router::{SelectedModel, TierRouter};
```

Update `src/lib.rs` exports:
```rust
pub use crate::tiers::{HealthConfig, ProviderHealthTracker, SelectedModel, TierConfigCache, TierRouter};
```
  </action>
  <verify>
`cargo check` passes.
All tiers types are exported from lib.rs.
  </verify>
  <done>
All tier routing types are properly exported.
Module structure is complete for Plan 03 integration.
  </done>
</task>

</tasks>

<verification>
1. `cargo check` - No compilation errors
2. `cargo test tiers::health::tests` - Health tracking tests pass
3. `cargo test tiers::router::tests` - Router tests pass
4. `cargo test routes::metrics::tests` - Metrics test passes
5. `cargo test` - Full test suite passes
</verification>

<success_criteria>
- ProviderHealthTracker implements exponential backoff (30s initial, 5min max, 2x multiplier)
- TierRouter uses cost-weighted selection (weight = 1/relative_cost)
- TierRouter filters unhealthy providers before selection
- TierRouter supports retry with get_retry_model
- ServiceUnavailable error returns 503 with Retry-After
- Prometheus metrics track tier requests, model selections, failures
- All types exported from lib.rs
</success_criteria>

<output>
After completion, create `.planning/phases/04-tier-routing/04-02-SUMMARY.md`
</output>
