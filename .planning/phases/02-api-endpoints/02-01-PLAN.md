---
phase: 02-api-endpoints
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/native_routes/mod.rs
  - src/native_routes/chat.rs
  - src/routes/mod.rs
autonomous: true

must_haves:
  truths:
    - "POST /native/v1/chat/completions accepts unified request format"
    - "Non-streaming request returns complete JSON response"
    - "Streaming request returns SSE chunks ending with [DONE]"
    - "Invalid requests return native error format with proper HTTP status"
    - "Authenticated requests pass through to OpenAI provider"
  artifacts:
    - path: "src/native_routes/mod.rs"
      provides: "Native API router with auth/rate-limit middleware"
      exports: ["create_native_router"]
    - path: "src/native_routes/chat.rs"
      provides: "Chat completions handler (streaming + non-streaming)"
      exports: ["native_chat_completions"]
  key_links:
    - from: "src/routes/mod.rs"
      to: "src/native_routes/mod.rs"
      via: "Router::nest('/native', native_routes::create_native_router)"
      pattern: "nest.*native.*create_native_router"
    - from: "src/native_routes/chat.rs"
      to: "src/native/translate/openai.rs"
      via: "OpenAITranslator for request/response translation"
      pattern: "OpenAITranslator::new"
    - from: "src/native_routes/chat.rs"
      to: "src/native/error.rs"
      via: "NativeErrorResponse for error returns"
      pattern: "NativeErrorResponse::"
---

<objective>
Create the /native/v1/chat/completions endpoint with support for both streaming and non-streaming responses.

Purpose: Expose unified Native API format to clients while translating to OpenAI provider format internally
Output: Working endpoint accepting ChatCompletionRequest and returning ChatCompletionResponse (or SSE stream)
</objective>

<execution_context>
@/Users/gregor/.claude/get-shit-done/workflows/execute-plan.md
@/Users/gregor/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Phase context - locked decisions
@.planning/phases/02-api-endpoints/02-CONTEXT.md
@.planning/phases/02-api-endpoints/02-RESEARCH.md

# Phase 1 types and translation (already built)
@src/native/mod.rs
@src/native/request.rs
@src/native/response.rs
@src/native/error.rs
@src/native/translate/openai.rs
@src/native/streaming.rs

# Existing routes pattern to follow
@src/routes/mod.rs
@src/routes/chat.rs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create native_routes module with router</name>
  <files>src/native_routes/mod.rs, src/lib.rs</files>
  <action>
Create the native_routes module that builds a Router for /native/* endpoints:

1. Create `src/native_routes/mod.rs`:
   - Declare `pub mod chat;`
   - Create `pub fn create_native_router(state: Arc<AppState>) -> Router`
   - Add route: `/v1/chat/completions` -> POST -> `chat::native_chat_completions`
   - Apply middleware in correct order (add rate_limit layer first, then auth layer - auth runs first at runtime)
   - Use existing `auth_middleware` and `rate_limit_middleware` - do NOT create new ones
   - Call `.with_state(state)` at the end

2. Add `pub mod native_routes;` to `src/lib.rs`

Pattern to follow (from 02-RESEARCH.md Pattern 1):
```rust
pub fn create_native_router(state: Arc<AppState>) -> Router {
    Router::new()
        .route("/v1/chat/completions", post(chat::native_chat_completions))
        .layer(middleware::from_fn_with_state(state.clone(), rate_limit_middleware))
        .layer(middleware::from_fn_with_state(state.clone(), auth_middleware))
        .with_state(state)
}
```
  </action>
  <verify>`cargo check` passes with new module</verify>
  <done>native_routes module exists with create_native_router function that compiles</done>
</task>

<task type="auto">
  <name>Task 2: Implement native chat completions handler</name>
  <files>src/native_routes/chat.rs</files>
  <action>
Create the chat completions handler that processes unified requests:

1. Create `src/native_routes/chat.rs` with `native_chat_completions` handler:

2. Handler signature:
```rust
pub async fn native_chat_completions(
    State(state): State<Arc<AppState>>,
    headers: HeaderMap,
    user: AuthenticatedUser,  // Extracted by auth middleware
    request: axum::extract::Request,
) -> Result<Response, NativeErrorResponse>
```

3. Request parsing:
   - Read body with `axum::body::to_bytes(request.into_body(), usize::MAX)`
   - Parse as `ChatCompletionRequest` using `serde_json::from_slice`
   - Map parse errors to `NativeErrorResponse::validation(msg)`

4. Request validation:
   - For Phase 2: Require `model` field (return validation error if None)
   - Document that Phase 4 makes model optional via tier routing

5. Translation:
   - Create `OpenAITranslator::new()`
   - Call `translator.translate_request(&native_request)` to get provider JSON
   - Map translation errors to `NativeErrorResponse::validation(e.to_string())`

6. Branch on streaming:
   - If `native_request.stream == true`: call `handle_streaming()`
   - Else: call `handle_non_streaming()`

7. Non-streaming flow (`handle_non_streaming`):
   - Forward translated request to `state.ai_provider.chat_completions()`
   - Translate response back with `translator.translate_response()`
   - Track usage via `state.batching_tracker.track()`
   - Return `Json(native_response)`

8. Streaming flow (`handle_streaming`):
   - Inject `stream_options.include_usage: true` into provider request (critical for token tracking)
   - Call `state.ai_provider.chat_completions_stream()`
   - Since OpenAI format IS our target format, stream passes through with minimal transformation
   - Use SSE headers from existing chat.rs (CONTENT_TYPE: text/event-stream, etc.)
   - Track usage from final chunk's usage field
   - Return `Response::builder().body(Body::from_stream(...))`

9. Error handling:
   - Provider errors: `NativeErrorResponse::provider_error(e.to_string(), "openai")`
   - Internal errors: `NativeErrorResponse::internal(msg)`

Key imports:
- `use crate::native::{request::ChatCompletionRequest, response::ChatCompletionResponse, error::NativeErrorResponse, translate::{OpenAITranslator, MessageTranslator}};`
- `use crate::middleware::auth::AuthenticatedUser;`
  </action>
  <verify>`cargo check` passes; handler compiles without warnings</verify>
  <done>native_chat_completions handler exists with streaming and non-streaming branches</done>
</task>

<task type="auto">
  <name>Task 3: Wire native routes into main router</name>
  <files>src/routes/mod.rs</files>
  <action>
Integrate native routes into the main application router:

1. Add import at top of `src/routes/mod.rs`:
   `use crate::native_routes;`

2. In `create_router()`, add after the `/v1` nest but BEFORE `.fallback()`:
```rust
    .nest("/native", native_routes::create_native_router(state.clone()))
```

3. Update the fallback message to mention /native:
   - Change "API endpoints are under /v1/" to "API endpoints are under /v1/ and /native/"

IMPORTANT: Use `nest()` not `merge()` - this keeps /native routes separate and doesn't interfere with existing /v1 routes.
  </action>
  <verify>
1. `cargo check` passes
2. `cargo test` passes (existing tests still work)
3. Manual verification: In logs, native routes should appear in route list
  </verify>
  <done>/native routes are wired and existing /v1 routes are unaffected</done>
</task>

</tasks>

<verification>
After all tasks complete:
1. `cargo check` - no warnings or errors
2. `cargo test` - all existing tests pass (regression-free)
3. `cargo build` - successful build
4. Code review checklist:
   - [ ] Native routes use existing middleware (not duplicated)
   - [ ] Error responses use NativeErrorResponse (not AppError)
   - [ ] Streaming injects stream_options.include_usage: true
   - [ ] Model field required (with comment about Phase 4)
</verification>

<success_criteria>
- POST /native/v1/chat/completions endpoint exists
- Handler correctly branches on stream: true/false
- OpenAI translator used for request/response conversion
- NativeErrorResponse used for all error returns
- Existing /v1/* tests continue to pass
- No new compiler warnings
</success_criteria>

<output>
After completion, create `.planning/phases/02-api-endpoints/02-01-SUMMARY.md`
</output>
